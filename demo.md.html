<meta charset="utf-8" lang="en">


<center style="margin-top: 350px;">
<span style="font-weight: bolder;">Procesamiento del Lenguaje de Signos</span><br>
<span style="font-weight: lighter; font-size: 35;">Fundamentos, tecnologías y retos actuales</span>
</center>

<div style="position: absolute; right: 50; top: 50; width: 25%;">
    <img src="videos/cover.gif">
</div>

<center style="margin-top: 200px;">
<span style="font-variant: small-caps; font-size: 30; color: rgb(163, 42, 42);">Claudio B. Apellido, J. Antonio Apellido, Julián M. Apellido, Víctor Ramos</span><br>
</center>

<center style="margin-top: 100px;">
    <span style="font-style: italic; font-size: 30;">Fuente Ppal: [Including Signed Languages in Natural Language Processing](https://aclanthology.org/2021.acl-long.570) (Yin et al., ACL 2021) </span><br>
    </center>


<div style="position:fixed; display :grid; grid-template-columns: 350px 1fr ; width:100%; bottom:0; right:0; font-size: large; text-align: right; font-style: italic; font-weight: bold; color: gray; padding-right: 30px;">
    <img style="float: left;" src="logo.png" width="350px"> <div style="display:flex; justify-content: end; align-items:end;">Procesamiento del Lenguaje Natural - Máster en Lógica Computación e Inteligencia Artificial - Universidad de Sevilla </div>
</div>

---

## Overview

<div style="display: flex; justify-content: center; align-items: center;"><video src="videos/Initial.mp4" style="width:90%" controls></video></div>

---


## Overview

<div style="display: grid; grid-template-columns: 1fr 1fr;">
<div>

1. Fundamentos de la Lengua de Signos.
    - Aspectos generales y motivación
    - Algunos aspectos lingüísticos
    - Métodos de Representación

</div>
<div>

2. Reconocimiento y traducción
    - Tecnologías y modos.
    - Detección
    - Identificación y Clasificación
    - Traducción

</div>
</div>
<div style="display: grid; grid-template-columns: 1fr 1fr;">
<div>

3. Producción 
    - Punto1
    - Punto2
    - Punto3
    - etc.

</div>
<div>

4. Datasets y Métricas de evaluación
    - Punto1
    - Punto2
    - Punto3
    - etc.

</div>
<div style="display: grid; grid-template-columns: 1fr 1fr; "></div>
<div style="grid-column-start: 2;">

5. El futuro de SLR junto a NLP. Conclusiones

</div>

</div>


---

# Lengua de Signos - Fundamentos

---


## Aspectos generales de las lenguas signadas (I)

* Las lenguas de signos son el principal método de comunicación para las personas sordas.

<center>
    <img src="images/usoLenguaSignosEnEuropa.jpg">
</center>

---

## Aspectos generales de las lenguas signadas (II). Mitos (I)

* Existe una única lengua de signos universal, conocida por todas las personas sordas.

<div style="display: flex; justify-content: space-around; height: 50%;">
    <img src="images/Sign-languages-in-the-world-based-on-Hammarstroem-et-al-2016.png"> <img src="images/familiasLenguasSignadas.webp">
</div>

<strong>Realidad</strong>: [Lista de lenguas de signos (Wikipedia)](https://en.wikipedia.org/wiki/List_of_sign_languages)

---

## Aspectos generales de las lenguas signadas (II). Mitos (II)

* Las lenguas de signos son traducciones letra a letra de cada una de las palabras de un idioma

<div style="display: flex; justify-content: space-around; height: 45%;">
    <img src="images/aslLetters.jpg"> <img src="images/aslWords.jpeg">
</div>


<strong>Realidad</strong>: Las lenguas de signos son lenguajes naturales completos, que involucran el uso de los gestos, las manos, el cuerpo y el entorno de los signantes. 

---

## Aspectos generales de las lenguas signadas (II). Mitos (II)

* Las lenguas de signos son traducciones letra a letra de cada una de las palabras de un idioma

<div style="display: flex; justify-content: space-around; height: 45%;">
    <img src="images/aslLetters.jpg"> <img src="images/aslWords.jpeg">
</div>


<strong>Realidad</strong>: Las lenguas de signos son lenguajes naturales completos, que involucran el uso de los gestos, las manos, el cuerpo y el entorno de los signantes. 

---


## Linguística de los Lenguajes de Signos

Las lenguas de signos constan de una estructura lingüística completa que cumple con los propósitos comunicativos de todo lenguaje natural. 

<center style="color: rgb(163, 42, 42);">¡Estructura más complicada que en los lenguajes orales!</center>


* <span style="color: rgb(163, 42, 42);">Fonología</span>: concreta las unidades mínimas e incluye gestos manuales y no manuales, movimientos, etc.
    


---

## Linguística de los Lenguajes de Signos

Las lenguas de signos constan de una estructura lingüística completa que cumple con los propósitos comunicativos de todo lenguaje natural. 

<center style="color: rgb(163, 42, 42);">¡Estructura más complicada que en los lenguajes orales!</center>


* <span style="color: rgb(163, 42, 42);">Fonología</span>: concreta las unidades mínimas e incluye gestos manuales y no manuales, movimientos, etc.
    

    <center style="font-size:150; margin-top: 120px;">
        <span style="color: rgb(163, 42, 42);">C</span>ASA vs <span style="color: rgb(163, 42, 42);">T</span>ASA</div>
    </center>

---

## Linguística de los Lenguajes de Signos

Las lenguas de signos constan de una estructura lingüística completa que cumple con los propósitos comunicativos de todo lenguaje natural. 

<center style="color: rgb(163, 42, 42);">¡Estructura más complicada que en los lenguajes orales!</center>


* <span style="color: rgb(163, 42, 42);">Fonología</span>: concreta las unidades mínimas e incluye gestos manuales y no manuales, movimientos, etc.
    

<div style="display: flex; justify-content: space-around; height: 40%;">
    <img src="images/DiferenciasSignos1.gif">
</div>

---


## Linguística de los Lenguajes de Signos

Las lenguas de signos constan de una estructura lingüística completa que cumple con los propósitos comunicativos de todo lenguaje natural. 

<center style="color: rgb(163, 42, 42);">¡Estructura más complicada que en los lenguajes orales!</center>


* <span style="color: rgb(163, 42, 42);">Fonología</span>: concreta las unidades mínimas e incluye gestos manuales y no manuales, movimientos, etc.
    

<div style="display: flex; justify-content: space-around; height: 40%;">
    <img src="images/DiferenciasSignos2.gif">
</div>

---


## Linguística de los Lenguajes de Signos

Las lenguas de signos constan de una estructura lingüística completa que cumple con los propósitos comunicativos de todo lenguaje natural. 

<center style="color: rgb(163, 42, 42);">¡Estructura más complicada que en los lenguajes orales!</center>


* <span style="color: rgb(163, 42, 42);">Fonología/Morfología</span>: concreta las unidades mínimas e incluye gestos manuales y no manuales, movimientos, ... y también el <span style="color: rgb(163, 42, 42);">Deletreo con los dedos</span>

* <span style="color: rgb(163, 42, 42);">Sintáxis</span>: En cada lengua de signos la sintáxis puede variar o incluso tener varias estructuras correctas. Especial relevancia : <span style="color: rgb(163, 42, 42);">Simultaneidad</span>

<center>
<div style="width:90%; display: grid; grid-template-columns: 1fr 1fr;">
    <div style="display: flex; flex-direction: column;">
        <div style="color: rgb(163, 42, 42);">YO NO TERMINAR LA PELÍCULA</div>
    </div> 
    <div style="display: flex; flex-direction: column;">
        <div style="color: rgb(163, 42, 42);">POR TI, PERRO MARRÓN YO RECOGER </div>
    </div> 
</div>

* <span style="color: rgb(163, 42, 42);">Semántica</span>: Establece el significado de los símbolos pero también posibles intenciones, emociones, etc. Aspecto clave: <span style="color: rgb(163, 42, 42);">Referencias</span>
</center>

--- 

## Linguística de los Lenguajes de Signos (II). Algunos aspectos clave.


<center>
    <div><video src="videos/vokoscreen-2022-04-24_17-44-43.mp4" style="width:55%" controls></video></div>
    <div><span style="font-style: italic; font-size: 20;"> Fuente: [ASL Conversation](https://www.youtube.com/watch?v=Ckccp6z_kbo) </span></div>
</center>

<center>
    <div style="width:90%; display: grid; grid-template-columns: 1fr 1fr 1fr;">
        <div style="display: flex; flex-direction: column;">
            <div style="color: rgb(163, 42, 42);">Simultaneidad</div>
        </div> 
        <div style="display: flex; flex-direction: column;">
            <div style="color: rgb(163, 42, 42);">Referencias</div>
        </div>
        <div style="display: flex; flex-direction: column;">
            <div style="color: rgb(163, 42, 42);">Deletreo con dedos </div>
        </div>
    </div>
    </center>

--- 

## Representación en lenguajes de signos


<center>
    <div style="width:90%; display: flex; flex-direction: column; justify-content: start;">
        <img src="images/Represent1.png">
    </div>
    </center>

---

## Representación en lenguajes de signos


<center>
    <div style="width:90%; display: flex; flex-direction: column; justify-content: start;">
        <img src="images/Represent1.png">
        <img src="images/Represent2.png">
    </div>
    </center>

---

## Representación en lenguajes de signos


<center>
    <div style="width:90%; display: flex; flex-direction: column; justify-content: start;">
        <img src="images/Represent1.png">
        <img src="images/Represent2.png">
        <img src="images/Represent3.png">
    </div>
    </center>

---

## Representación en lenguajes de signos


<center>
    <div style="width:90%; display: flex; flex-direction: column; justify-content: start;">
        <img src="images/Represent1.png">
        <img src="images/Represent2.png">
        <img src="images/Represent3.png">
        <img src="images/Represent4.png">
    </div>
</center>
    
Recursos disponibles:  <span style="color: rgb(163, 42, 42);">Diccionarios</span>, <span style="color: rgb(163, 42, 42);">Símbolos aislados</span>, <span style="color: rgb(163, 42, 42);">Deletreo manual</span>, <span style="color: rgb(163, 42, 42);">Signos continuos</span>.

        
    Un problema añadido: <span style="color: rgb(163, 42, 42);">Anonimización</span>
  

---

## Objetivos en SLP

    
<div style="display: grid; grid-template-columns: 1fr 1fr; margin-top: 50px;">
    <img src="videos/goals1.gif"> 
    <img src="videos/goals2.gif">
</div>  

Un sistema traductor: Lenguaje signado \(\longleftrightarrow\) Lenguaje Oral involucra varias tareas: 

<center style="color: rgb(163, 42, 42);">
Detección y reconocimiento, Traducción y Producción.
</center>


---

# Tareas en Lenguas de Signos I. <br> Reconocimiento y traducción

---

## Reconocimiento (SLR)

Tarea de reconocer los propios elementos discretos del lenguaje de signos, que incluye todo el proceso de seguimiento e identificación
de los signos realizados y su conversión en palabras y expresiones semánticamente significativas.


* <span style="font-weight: bold;">Tecnología</span>:  <span style="color: rgb(163, 42, 42);"> Aproximaciones basadas sólo en imagen </span> (con uso exclusivo de cámaras y procesamiento basado en Visión por Computador) / <span style="color: rgb(163, 42, 42);">Aproximaciones basadas en multi-sensores</span> (se cuenta con componentes adicionales: guantes, acelerómetros,... procesando la información de forma conjunta).

* <span style="font-weight: bold;">Objetivo</span>:  <span style="color: rgb(163, 42, 42);">Reconocimiento aislado</span> (reconocimiento de cada uno de los símbolos por separado) / <span style="color: rgb(163, 42, 42);">Reconocimiento continuo</span> (la detección y clasificación se realiza a partir de varios símbolos a través del tiempo).

--- 


## Reconocimiento. Métodos basados en visión por computador (VBA).

*  <span style="color: rgb(163, 42, 42);">Vídeo</span> como fuente para el reconocimiento.

*  Desarrollo de modelos y capacidad computacional en visión artificial en la última década: aprovechamiento de la <span style="color: rgb(163, 42, 42);">información temporal</span>.

* En general, se preprocesan las imágenes adquiridas para obtener un vector de características y se clasifica según el modelo usado.

<div style="display: grid; grid-template-columns: .5fr .5fr; margin-top: 20px;">
    <div align="left"><video src="videos/VBA_1.mp4" style="width:80%" controls></video></div>
	<div align="right"><video src="videos/VBA_2.mp4" style="width:80%" controls></video></div>
</div>  
<center>
    <div><span style="font-style: italic; font-size: 20;"> Fuente: [Nicholas Renotte](https://www.youtube.com/watch?v=doDUihpj6ro) </span></div>
</center>

--- 

## Reconocimiento. Aproximaciones basadas en guantes (GBA).

* Captura de posición, orientación y ubicación de las manos con alta precisión.

<center>
    <div><video src="videos/GBA.mp4" style="width:55%" controls></video></div>
    <div><span style="font-style: italic; font-size: 20;"> Fuente: [Tech Insider](https://www.youtube.com/watch?v=RHTrAXsULOI&ab_channel=TechInsider) </span></div>
</center>

--- 

## Reconocimiento. Reconocimiento Aislado vs Reconocimiento continuo

Actualmente la mayoría de los modelos de reconocimiento se dedican al reconocimiento asilado, eso es, al reconocimiento de cada signo por separado. Varias limitaciones:

* Pérdida del contexto, especialmente grave en el tratamiento de las Referencias
* Problemas de análisis temporal diferenciación de signos por movimiento: Oclusión, Tracking, ... 
* Dificultad en el empleo de técnicas basadas en lenguaje en vez de en visión 

Nuevas líneas: <span style="color: rgb(163, 42, 42);">Análisis continuo</span>. Con gran influencia de técnicas de <u>análisis temporal</u>: Hidden Markov Models (HMM), Dynamic Time Wrapping, Conditional Random Fields (CRF), Support Vector Machines (SVM), Deep Learning (3D-CNN-LSTM, ...)

--- 

## Detección.

Tarea de clasificación binaria de cualquier fotograma dado de un vídeo que verifica si una persona está utilizando
lenguaje de signos o no.

* <span style="color: rgb(163, 42, 42);">Borg y Camilleri. 2019</span>: Procesamiento de los fotogramas en bruto basado en redes convolucionales.

* <span style="color: rgb(163, 42, 42);">Moryossef et al. 2020</span>: Detección a partir de la pose estimada y no del fotograma en bruto.

<center>
<div style=" width: 75%; display: flex; flex-direction: row; justify-content: center;">
    <img src="images/detection_2.png" style="width:45%"> 
    <img src="images/detection_1.png" style="width:42%"> 
</div> 
</center>

Problema: <span style="color: rgb(163, 42, 42);">Insuficientes datasets correctamente anotados para evaluación de datos reales.</span>


--- 


## Identificación.

La tarea de identificación de lenguas de signos se define como la clasificación entre dos o más lenguas de signos.

* <span style="color: rgb(163, 42, 42);">Gebre, Wittenburg y Heskes.2013</span>: un clasificador random forest simple puede distinguir entre la lengua de signos británica (BSL) y la lengua de signos griega(ENN) con F1 del 95%.

* <span style="color: rgb(163, 42, 42);">Monteiro et al. 2016</span>: Misma arquitectura. Diferencia entre la lengua de signos británica y la lengua de signos francesa con F1 de 98% de en vídeos con fondos estáticos, y entre la lengua de signos americana y británica con un 70%.


Atribuyen su éxito principalmente a los diferentes sistemas de deletreo con los dedos, que es a dos manos en el caso británico y a una mano en el caso estadounidense.


--- 


## Segmentación.

Detección de los límites de los signos en un fotograma u oraciones para dividirlos en unidades lingüísticas con significado.

* <strong>Lenguaje hablado</strong>: División en el tiempo como secuencia lineal.

<center style="color: rgb(163, 42, 42);">¡Simultaneaidad del lenguaje signado!</center>

* <strong>Lenguaje signado</strong>: Concepto de palabra difuso. Aproximación lineal insuficiente.

* <span style="color: rgb(163, 42, 42);">Farag y Brock. 2019</span>: Detección de los límites de las palabras dentro de expresiones en el lenguaje de signos japonés a partir de las posiciones tridimensionales de las articulaciones del cuerpo.

* <span style="color: rgb(163, 42, 42);">Bull, Gouiffès y Braffort. 2020</span>: Segmentación de lengua de signos francesa que han sido correctamente subtitulados a partir de la alienación de los mismo.

--- 

## Segmentación.

* <span style="color: rgb(163, 42, 42);">Gül Varol et al. 2021</span>:Mejoran las métricas obtenidas por Farag y Brock con una arquitectura que acopla una red convolucional espacio-temporal 3D (I3D) con una red convolucional temporal multietapa (1D).

<center>
    <div style="width:66%; display: flex; flex-direction: column; justify-content: start;">
        <img src="images/segmentation.png">
		<div><span style="font-style: italic; font-size: 20;"> Fuente: Gül Varol et al. 2021 </span></div>
    </div>
</center>


--- 

## Framework general.
El reconocimiento del lenguaje de signos consta de tres fases principales: 

* Segmentación de imagen y seguimiento.
* Extracción de características.
* Clasificación. 

<center>
    <div style="width:58%; display: flex; flex-direction: column; justify-content: start;">
        <img src="images/general_framework.png">
		<div><span style="font-style: italic; font-size: 20;"> Fuente: Subhash Chand Agrawal et al. 2016 </span></div>
    </div>
</center>

---

## Traducción

La RAE lo define como: Acción y efecto de expresar en una lengua lo que está escrito o se ha expresado antes en otra.

    * En el contexto SLP: tarea de conversión del lenguajes de signos interpretados a otros lenguajes, no necesariamente de signos.
	
<center>
    <div style="width:50%; display: flex; flex-direction: column; justify-content: start;">
        <img src="images/translation.png">
		<div><span style="font-style: italic; font-size: 20;"> Fuente: [https://sign-language-processing.github.io/](Sign Language Processing) </span></div>
    </div>
</center>

---
## Traducción automática (MT)
    * Invisibilización de la importancia del lenguaje de signos.
    * Planteamiento muy diferente al habitual, pues trabajamos con vídeos en lugar de con audio o texto escrito.
Actualmente los modelos que mejores resultados obtienen provienen de técnicas hibrídas entre los dos campos de gran desarrollo actualmente : Visión y PLN. **¡ÉSTE ES EL FUTURO DEL CAMPO!**
<center>
    <div style="width:40%; display: flex; flex-direction: column; justify-content: start;">
        <img src="images/translation2.png">
		<div><span style="font-style: italic; font-size: 20;"> Fuente: [Sign language transformers](https://openaccess.thecvf.com/content_CVPR_2020/papers/Camgoz_Sign_Language_Transformers_Joint_End-to-End_Sign_Language_Recognition_and_Translation_CVPR_2020_paper.pdf) </span></div>
    </div>
</center>

--- 

## Traducción: glosa a texto

Tarea que busca convertir glosa escrita en lenguaje natural escrito.

	* Difieren tanto en la terminología como en la estructura.
	* Ejemplo en ASL: inversión Sujeto-Verbo-Objeto (SVO), pasando a ser Objeto-Sujeto-Verbo (OSV).

<center>
    <table>
        <tr>
          <th>Glosa</th>
          <th></th>
          <th>Texto</th>
       </tr>
        <tr>
          <td>PRONOMBRE-2 SABER SIGNAR</td>
          <td>    →    </td>
          <td>¿Sabes hablar lenguaje de signos?</td>
        </tr>
      </table>     
</center>

    * <span style="color: rgb(163, 42, 42);">Cihan Camgöz et al. 2018</span>  Comparan arquitercutas LTSM (Long short-term memory) y GRU (gated recurrent unit) junto con mecanismos de atención.
    * <span style="color: rgb(163, 42, 42);">Yin y Read. 2020</span> Siguiendo los avances en materia de traducción automática del lenguaje hablado, propusieron cambiar la red neuronal con un transformador mostrando mejoras con respecto a trabajos previos tanto en lenguaje de signos alemán, como en el ASL.

--- 

## Traducción: texto a glosa

* <span style="color: rgb(163, 42, 42);">Zhao et al. 2000</span> Sistema basado en gramáticas de adjunción de árboles (TAG) para traducir frases en inglés a ASL. 
	* Se genera un árbol de glosas para el ASL.
	* Empareja los árboles elementales del ASL con los árboles elementales del inglés.
	* Se realizan sustituciones y combinaciones de nodos.
	* Primera ocasión en la que se pudo ver TAG aplicado a un lenguaje de signos.

* <span style="color: rgb(163, 42, 42);">Othman y Jemni. 2012</span> Identificaron la necesidad existente de un corpus paralelo de glosa y texto de una dimensiones elevadas. Diseñaron una gramática con el objetivo de convertir frases en inglés a glosa del ASL. 
	* 100 millones de frases sintéticas.
	* 800 millones de palabras.
	* Difícil medir la calidad del corpus, dado que no se llegó a evaluar el método de producción en parejas reales inglés-ASL.

---

## Traducción: vídeo a texto

* <span style="color: rgb(163, 42, 42);">Camgöz et al. 2020b</span>
	* Proponen arquitectura basada en transformadores que codifican cada fotograma.
	* Usa referencias tanto de glosas como de texto.
	* Usan un trasformador para decodificar el lenguaje hablado.

* <span style="color: rgb(163, 42, 42);">Camgöz et al. 2020a</span>
	* Arquitectura que no depende de las glosas.
	* Recorta la mano y cara del signante y estiman la pose 3D mediante transformadores.

---


## Traducción: vídeo a pose

* <span style="color: rgb(163, 42, 42);">OpenPose</span> Primer sistema capaz de reconocer, conjuntamente, las posiciones clave de los cuerpos, caras, manos y pies.
	* 135 ubicaciones en 2D en cada imagen. 
	* Puede estimar la pose directamente a partir de una única inferencia. 
	* Se estima la pose del cuerpo e independientemente de las manos y de la cara.
	* Capaz de triangular otras posiciones clave dados diferentes ángulos de grabación, siendo capaz de reconstruir la pose en 3D.

* <span style="color: rgb(163, 42, 42);">DensePose</span>
	* Sustituyen la clasificación de cada posición clave por una segmentación semántica según la parte del cuerpo.
	* Predice si dicho píxel puede ubicarse en una proyección 2D del modelo. 
	* Reconstrucción a cuerpo completo.
	* No tiene en cuenta la información temporal del movimiento. Problema con el desenfoque (blur).

---

## Traducción: vídeo a pose

* <span style="color: rgb(163, 42, 42);">MediaPipe Holistic.</span>
	* Aproximación semejante a la de OpenPose.
	* Poses estimadas eran tridimensionales.	
	* Capaz de ejecutarse a tiempo real en una CPU.
	* Disponible para Android, iOS, C++, Pythony es ampliamente utilizada.
<center>
    <div style="width:40%; display: flex; flex-direction: column; justify-content: start;">
        <img src="images/holistic.gif">
        <div><span style="font-style: italic; font-size: 20;"> Fuente: Holistic </span></div>
    </div>
</center>

---

## Traducción: estado del arte

* <span style="color: rgb(163, 42, 42);">VAC (Visual Alignment Constraint for Continuous Sign Language Recognition).</span>
	* Dos funciones de pérdida, una centrada en características visuales, y otra dedicada para el módulo de alineación.
<center>
    <div style="width:60%; display: flex; flex-direction: column; justify-content: start;">
        <img src="images/vac.png">
    </div>
</center>

---

## Traducción: estado del arte

* <span style="color: rgb(163, 42, 42);">TSPNet (Hierarchical Feature Learning via Temporal Semantic Pyramid for Sign Language Translation).</span>
    * Consideran información temporal como añadido de cara al entrenamiento frente tratamiento frame a frame.
<center>
    <div style="width:80%; display: flex; flex-direction: column; justify-content: start;">
        <img src="images/TSPNet.png">
    </div>
</center>


---

## Traducción: estado del arte

* <span style="color: rgb(163, 42, 42);">SAM-SLR (Skeleton Aware Multi-modal Sign Language Recognition).</span>
    * Se centra en la complejidad y la rapidez de los movimientos realizados por una persona signando.
	* Redes convolucionales para extraer las posiciones principales de las manos. 
<center>
    <div style="width:50%; display: flex; flex-direction: column; justify-content: start;">
        <img src="images/sam_slr.jpg">
    </div>
</center>


---

# Tareas en Lenguas de Signos III. <br> Producción.

--- 

## Producción

--- 

## Inclusión de SLP en el marco NLP

--- 

#  Estado del arte. <br> Modelos propuestos, Métricas y Datasets.

--- 

## Modelos propuestos

--- 

## Datasets más frecuentes

Primeramente, debemos destacar la labor de Duarte et al. [42] con la creación del repositorio que
usaremos como principal referencia, en el que podemos encontrar multitud de conjuntos de datos, así
como las características de cada uno, tales como el idioma, el tamaño de los archivos, la disponibilidad...
En definitiva, con este recurso contamos con una amplia biblioteca de conjuntos de datos útiles para
el estudio del lenguaje de signos.

<object type="text/html" data="https://how2sign.github.io/related_datasets.html" width="100" height="100"></object>

--- 

## Datasets más frecuentes

* <span style="color: rgb(163, 42, 42);">RWTH-PHOENIX-Weather 2014 (Continuous Sign Language Recognition Dataset)
    y 2014 T (Parallel Corpus of Sign Language Video, Gloss and Translation).</span>
    Diseñados por Koller et al. [43] y Camgöz et al. [44], estos conjuntos de datos se generaron a partir
    de los noticieros del tiempo durante los años entre 2009 y 2011, diariamente y en alemán, en el
    canal de televisión pública PHOENIX, con la interpretación del lenguaje de signos. Actualmente,
    solo un número reducido de ediciones han sido transcritas usando glosa. Estas transcripciones
    fueron escritas por nativos del GSL. Adicionalmente, el pronóstico del tiempo fue transcrito
    de forma semiautomática recurriendo a un sistema de reconocimiento del lenguaje (RASR).
    Adicionalmente, el conjunto de datos se completó añadiendo alemán hablado para capturar la
    variabilidad en la traducción.
    El signado se grabó mediante una cámara en frente de los intérpretes, vistiendo estos ropas negras
    frente a un fondo artificial gris con transición de color.
    Por su parte, el 2014 T incluye contenido adicional, más concretamente las anotaciones originales
    de los ficheros en formato XML.

--- 

## Datasets más frecuentes

* <span style="color: rgb(163, 42, 42);">AUTSL (A Large Scale Multi-Modal Turkish Sign Language Dataset and Baseline).</span>
Diseñado por Sincan y Keles, este conjunto de datos presenta vídeos de lenguaje de signos
turco (TSL) con el objetivo de proporcionar un punto de partida para futuras evaluaciones de
modelos. El resultado cuenta con 226 signos interpretados por 43 signantes diferentes y un total
de 38.336 muestras aisladas de signos, todos ellos con diferentes fondos, posiciones y posturas.
Se incluye, además del RGB, la profundidad y la pose del esqueleto. Recurriendo a CNNs y
LSTM fueron capaces de proporcionar los primeros resultados sobre el conjunto. Es reseñable ver
que modelos diseñados por los autores lograron una precisión del 95 % aproximadamente en los
conjuntos AUTSL y Montalbano, obteniendo sin embargo un 62.02 % en el propio AUTSL para
una subselección realizada por un usuario independiente, mostrando de esta forma los nuevos
retos que introdujo el conjunto de datos en el sector.

--- 

## Datasets más frecuentes

* <span style="color: rgb(163, 42, 42);">WLASL-2000 (Word-level Deep Sign Language Recognition from Video: A New
    Large-scale Dataset and Methods Comparison).</span>
    El conjunto de datos proporcionado en [45] es el mayor a nivel de palabras dentro del ASL,
contando con 2.000 palabras diferentes comunes en dicho lenguaje. Los creadores diseñaron el
conjunto con la esperanza de facilitar la investigación en el área de la comprensión del lenguaje
de signos, buscando favorecer eventualmente la comunicación entre comunidades sordas y oyentes.

* <span style="color: rgb(163, 42, 42);">LSA64 (LSA64: A Dataset for Argentinian Sign Language).</span>
Este conjunto cuenta con 3.200 vídeos en lenguaje de signos argentino (LSA), con 10 intérpretes
no expertos ejecutan cada uno 5 repeticiones de 64 tipos de signos. Los signos fueron seleccionados
de entre los más comúnmente utilizados en el léxico del LSA, incluyendo verbos y sustantivos

--- 

## Datasets más frecuentes: problemáticas

    * <span style="color: rgb(163, 42, 42);">Escasez</span>

--- 

## Datasets más frecuentes: problemáticas

    * <span style="color: rgb(163, 42, 42);">Escasez</span>
    * <span style="color: rgb(163, 42, 42);">Incompletitud</span>

--- 

## Datasets más frecuentes: problemáticas

    * <span style="color: rgb(163, 42, 42);">Escasez</span>
    * <span style="color: rgb(163, 42, 42);">Incompletitud</span>
    * <span style="color: rgb(163, 42, 42);">Incompatibilidad</span>

--- 

## Datasets más frecuentes: problemáticas

    * <span style="color: rgb(163, 42, 42);">Escasez</span>
    * <span style="color: rgb(163, 42, 42);">Incompletitud</span>
    * <span style="color: rgb(163, 42, 42);">Incompatibilidad</span>
    * <span style="color: rgb(163, 42, 42);">Anonimato</span>

--- 

## Métricas utilizadas

    * <span style="color: rgb(163, 42, 42);">WER, Word Error Rate</span>
    * <span style="color: rgb(163, 42, 42);">BLEU</span>
    * <span style="color: rgb(163, 42, 42);">ROUGE</span>

--- 

## Métricas utilizadas: WER

WER, Word Error Rate, es una medida comúnmente utilizada en la evaluación de sistemas de reconocimiento
del habla o de traducción automática.
Calcula el número mínimo de inserciones, borrados y sustituciones de una palabra por otra, necesarios
para transformar una frase en otra. Esta medida se basa en la distancia de edición o distancia de
Levenshtein, con la salvedad de que esta última se calcula a nivel de letra y WER lo hace a nivel de
palabra.
En tareas tanto de traducción automática como de reconocimiento del habla, se calcula WER entre la
frase generada por el sistema y una frase de referencia correcta.
<img src="images/WER.PNG" alt="Fórmula WER">
    * S es el número de sustituciones,
    * B es el número de borrados,
    * I es el número de inserciones,
    * N es el número de palabras que tiene la frase de referencia.
Word error rates WER measure string similarity based on word-level Levenshtein distance calculations
[KP02]. A reference (ground truth) is compared to a hypothesis (system prediction) by calculating
necessary operations, transforming a hypothesis into reference sentence. A common approach of calculating
WER is described in equation 4.9.

--- 

## Métricas utilizadas: BLEU-n

BLEU (Bilingual Evaluation Understudy) es un método de evaluación de la calidad de traducciones
realizadas por sistemas de traducción automática. Una traducción tiene mayor calidad cuanto más
similar es con respecto a otra de referencia, que se supone correcta. BLEU puede calcularse utilizando
más de una traducción de referencia. Esto permite una mayor robustez a la medida frente a traducciones
libres realizadas por humanos.
BLEU se calcula normalmente a nivel de frases y halla la precisión en ngramas entre la traducción del
sistema y la de referencia. Sin embargo, se utiliza una precisión modificada con el fin de solucionar
ciertas deficiencias en la medida.

BLEU, or the Bilingual Evaluation Understudy, is a score for comparing a candidate translation of
text to one or more reference translations.
Although developed for translation, it can be used to evaluate text generated for a suite of natural
language processing tasks

<img src="images/BLEU.PNG" alt="Fórmula BLEU">

Habitualmente BLEU-4

--- 

## Métricas utilizadas: ROUGE

As opposed to the BLEU score, the Recall-Oriented Understudy for Gisting Evaluation (ROUGE)
evaluation metric measures the recall. It’s typically used for evaluating the quality of generated text
and in machine translation tasks — However, since it measures recall it’s mainly used in summarization
tasks since it’s more important to evaluate the number of words the model can recall in these types of
tasks.

ROUGE, or Recall-Oriented Understudy for Gisting Evaluation,[1] is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.


The following five evaluation metrics are available.

    * ROUGE-N: Overlap of n-grams[2] between the system and reference summaries.
        * ROUGE-1 refers to the overlap of unigram (each word) between the system and reference summaries.
        * ROUGE-2 refers to the overlap of bigrams between the system and reference summaries.
    * ROUGE-L: Longest Common Subsequence (LCS)[3] based statistics. Longest common subsequence problem takes into account sentence level structure similarity naturally and identifies longest co-occurring in sequence n-grams automatically.
    * ROUGE-W: Weighted LCS-based statistics that favors consecutive LCSes .
    * ROUGE-S: Skip-bigram[3] based co-occurrence statistics. Skip-bigram is any pair of words in their sentence order.
    * ROUGE-SU: Skip-bigram plus unigram-based co-occurrence statistics.

--- 


# Futuro del SLP y conclusiones

---

## Inclusión de SLP en el marco NLP

Aunque las lenguas de signos y las habladas difieren en cuanto a la modalidad, dado que ambas expresan los niveles linguísticos propios de los lenguajes naturales, las teorías fundamentales en NLP pueden y deben extenderse a las lenguas de signos.

* <span style="color: rgb(163, 42, 42);">Herramientas de NLP</span>: 
	* Tokenizadores.
	* Analizadores sintacticos.
	* Reconocimiento de entidades nombradas.
	* Resolución de correferencia.

--- 

## Inclusión de SLP en el marco NLP

* <span style="color: rgb(163, 42, 42);">Tokenizadores</span>
	* La gran mayoría de los métodos de PLN requieren una entrada  discreta (token).
	* Necesidad de herramientas de tokenización adecuadas que mapeen los vídeos de las lenguas de signos a una representación discreta y precisa con una mínima pérdida de información.
	* Los sistemas y conjuntos de datos de PLN existentes suelen utilizar glosas como unidades léxicas discretas. Problemas:
		* Insuficientes para las construcciones espaciales de la lengua de signos.
		* Glosas específicas de cada lengua, no estandarizadas.


<span style="font-weight: bold;"> Preguntas abiertas:</span>

- ¿Cómo definimos las unidades léxicas en las lenguas de signos? 
- ¿Las unidades fonológicas de las lenguas de signos pueden asignarse a unidades léxicas?
- ¿Pueden las técnicas utilizadas en *Speech Recognition* ser aplicadas las lenguas de signos?

--- 

## Inclusión de SLP en el marco NLP

* <span style="color: rgb(163, 42, 42);">Analizadores sintácticos</span>
	* El etiquetado de partes del discurso (PDD) y el análisis sintáctico son fundamentales para comprender el significado de las palabras en contexto.
	* Se debe definir en qué medida el etiquetado PDD y el análisis sintáctico para los idiomas hablados también se generalizan a los lenguajes de signos.

<span style="font-weight: bold;"> Preguntas abiertas:</span>

- ¿Existen teorías lingüísticas para diseñar características y reglas que realicen estas tareas? 
- ¿Cómo se expresan las características morfológicas?
- ¿Necesitamos un nuevo conjunto de etiquetas y PDD para los lenguajes de signos? 

--- 

## Inclusión de SLP en el marco NLP

* <span style="color: rgb(163, 42, 42);">Reconocimiento de entidades nombradas</span>
	* Las entidades nombradas en los lenguajes de señas pueden producirse mediante una secuencia de deletreo manual, un signo o incluso mediante la pronunciación del nombre mientras que la referencia es señalanda..
	* Un detalle importante: **La Anonimización**.

<span style="font-weight: bold;"> Preguntas abiertas:</span>

- ¿Cuáles son los marcadores visuales de las entidades nombradas?
- ¿Cómo se presentan y referencian?
- ¿Cómo se establecen las relaciones entre ellos?

--- 

## Hacia una PNL lingüísticamente informada y multimodal

Es necesaria la colaboración de las comunidades de investigación multimodal y de SLP para desarrollar modelos potentes de SLP apoyados herramientas básicas de PLN como las comentadas, todo ello mientras se procesa y relaciona la información procedente de las modalidades lingüística y visual

El SLP está especialmente sujeta a tres de los principales retos técnicos en el aprendizaje automático multimodal:

<span style="font-weight: bold;"> Puntos clave:</span>

* <span style="color: rgb(163, 42, 42);">Traducción</span>: ¿cómo pasar la información visual-gestual a/desde la información oral-textual?
* <span style="color: rgb(163, 42, 42);">alineación</span>: ¿cómo relacionar las unidades de la lengua de signos con las de la lengua hablada?
* <span style="color: rgb(163, 42, 42);">co-aprendizaje</span>: ¿cómo aprovechar el conocimiento de la lengua hablada en la lengua de signos?


--- 

## Conclusiones finales

<center style="color: rgb(163, 42, 42); font-family: 'Lucida Console', 'Courier New', monospace;">Xopre?</center>

---



# Referencia markdeep-HTML-cositas

--- 

## Anotaciones Xopre (básicas)

Lista (y esto es texto normal):

* <strong>Negrita con flechita</strong>

<center style="color: rgb(163, 42, 42);">Rojo centrado</center>

- Elemento de lista sin la flechita monísima

    1. Elemento enumerado
        - Enumerado? No
    2. Elemento enumerado
    - Elemento autoenumerado


--- 

## Anotaciones Xopre (vídeo con controladores)

<center>
    <div><video src="videos/GBA.mp4" style="width:55%" controls></video></div>
    <div><span style="font-style: italic; font-size: 20;"> Fuente: [Tech Insider](https://www.youtube.com/watch?v=RHTrAXsULOI&ab_channel=TechInsider) </span></div>
</center>

--- 

## Anotaciones Xopre (gifs)

<center>
    <div style="width:30%; display: flex; flex-direction: column; justify-content: start;">
        <img src="videos/cover.gif">
		<div><span style="font-style: italic; font-size: 20;"> Fuente: [Sign language transformers](https://openaccess.thecvf.com/content_CVPR_2020/papers/Camgoz_Sign_Language_Transformers_Joint_End-to-End_Sign_Language_Recognition_and_Translation_CVPR_2020_paper.pdf) </span></div>
    </div>
</center>

<!-- <div style="position: absolute; right: 50; top: 50; width: 25%;">
    <img src="videos/cover.gif">
</div> -->

--- 


## Anotaciones Xopre (2 vídeos)

<div style="display: grid; grid-template-columns: .5fr .5fr; margin-top: 20px;">
    <div align="left"><video src="videos/goals1.gif" style="width:80%" controls></video></div>
	<div align="right"><video src="videos/VBA_2.mp4" style="width:80%" controls></video></div>
</div>  
<center>
    <div><span style="font-style: italic; font-size: 20;"> Fuente: [Nicholas Renotte](https://www.youtube.com/watch?v=doDUihpj6ro) </span></div>
</center>

---



<center style="margin-top: 350px;">
    <span style="font-weight: bolder;">Procesamiento del Lenguaje de Signos</span><br>
    <span style="font-weight: lighter; font-size: 35;">Fundamentos, tecnologías y retos actuales</span>
    </center>
    
    <center style="margin-top: 200px;">
    <span style="font-variant: small-caps; font-size: 30; color: rgb(163, 42, 42);">Claudio B. Apellido, J. Antonio Rodríguez, Julián M. Apellido, Víctor Ramos</span><br>
    </center>
    
    <center style="margin-top: 100px;">
        <span style="font-style: italic; font-size: 30;">Fuente Ppal: [Including Signed Languages in Natural Language Processing](https://aclanthology.org/2021.acl-long.570) (Yin et al., ACL 2021) </span><br>
        </center>

<!-- Markdeep slides stuff -->
<script>
    markdeepSlidesOptions = {
        aspectRatio: 16 / 9,
        theme: 'simple',
        fontSize: 22,
        diagramZoom: 1.0,
        totalSlideNumber: false,
        progressBar: true,
        breakOnHeadings: false,
        slideChangeHook: (oldSlide, newSlide) => {},
        modeChangeHook: (newMode) => {}
    };
</script>
<link rel="stylesheet" href="markdeep-slides/lib/markdeep-relative-sizes/1.11/relativize.css">
<link rel="stylesheet" href="markdeep-slides/markdeep-slides.css">
<script src="markdeep-slides/markdeep-slides.js"></script>

<!-- Markdeep stuff -->
<script>
    markdeepOptions = {
        tocStyle: 'none',
        detectMath: true,
        onLoad: function() {
            initSlides();
        }
    };
</script>
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
<script src="markdeep-slides/lib/markdeep/1.11/markdeep.min.js" charset="utf-8"></script>
<script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
